{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing (NLP) Tutorial\n",
    "\n",
    "In this tutorial, we will talk about natural language processing (NLP) using Python. This NLP tutorial will use Python NLTK library. NLTK is a popular Python library which is used for NLP.\n",
    "\n",
    "So what is NLP? and what are the benefits of learning NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is NLP?\n",
    "\n",
    "Simply and in short, natural language processing (NLP) is about developing applications and services that are able to understand human languages.\n",
    "\n",
    "We are talking here about practical examples of natural language processing (NLP) like understanding synonyms of matching words, and writing complete grammatically correct sentences and paragraphs. This is not everything, you can think about the industrial implementations about these ideas and its benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Benefits of NLP\n",
    "\n",
    "As all of you know, there are millions of gigabytes every day are generated by blogs, social websites, and web pages.\n",
    "\n",
    "There are many companies gathering all of these data for understanding users and their passions and give these reports to the companies to adjust their plans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NLP Implementations\n",
    "\n",
    "These are some of the successful implementation of Natural Language Processing (NLP):\n",
    "- **Search engines** like Google, Yahoo, etc. \n",
    "- **Social websites feeds** like Facebook news feed.\n",
    "- **Speech engines** like Apple Siri.\n",
    "- **Spam filters** like Google spam filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NLP Libraries\n",
    "\n",
    "There are many open source Natural Language Processing (NLP) libraries and these are some of them:\n",
    "\n",
    "- [Natural language toolkit (NLTK)](https://www.nltk.org/) (Python)\n",
    "- [StanfordNLP](https://stanfordnlp.github.io/stanfordnlp/index.html) (Python)\n",
    "- [spaCy](https://spacy.io/)  (Python)\n",
    "- [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/)  (Java)\n",
    "- [Apache OpenNLP](https://opennlp.apache.org/) (Java)\n",
    "\n",
    "In this NLP Tutorial, we will use Python NLTK library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Install NLTK\n",
    "\n",
    "In Jupyter notebook, you can install it by (only run once):\n",
    "```python\n",
    "!pip install -U NLTK\n",
    "```\n",
    "Download nltk data to support some NLP functionalities (only run once).\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# install NLTK by (only run once):\n",
    "!pip install -U NLTK\n",
    "\n",
    "# Download nltk data to support some NLP functionalities (only run once).\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Text data overview\n",
    "\n",
    "The SMS Spam Collection v.1 is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The files contain one message per line. Each line is composed by two columns: one with label (ham or spam) and other with the raw text, separated by `TAB` or `\\t`. Here are some examples:\n",
    "\n",
    "```\n",
    "ham   What you doing?how are you?\n",
    "ham   dun say so early hor... U c already then say...\n",
    "ham   MY NO. IN LUTON 0125698789 RING ME IF UR AROUND! H*\n",
    "spam   FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now!\n",
    "spam   Sunshine Quiz! Win a super Sony DVD recorder if you canname the capital of Australia?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read training data\n",
    "train_file = \"https://github.com/liuhoward/teaching/raw/master/big_data/smsspam/SMSSpamCollection.train\"\n",
    "train_data = pd.read_csv(train_file, sep='\\t', header=None, names=['label', 'text'])\n",
    "\n",
    "print(f'num train records: {len(train_data)}')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# simple example to split text with space\n",
    "text = \"What you doing?how are you?\"\n",
    "tokens = [t for t in text.split(' ')]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We saw how to split the text into tokens using split function, now we will see how to tokenize the text using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# simple example to tokenize using NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"What you doing?how are you?\"\n",
    "# use lower case\n",
    "text = text.lower()\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# a simple example to calculate frequency\n",
    "import nltk\n",
    "\n",
    "freq = nltk.FreqDist(tokens)\n",
    "print(freq.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize training text\n",
    "train_texts = train_data['text']\n",
    "print(len(train_texts))\n",
    "train_text_tokens = [word_tokenize(text.lower())   for text in train_texts]\n",
    "# print the first example\n",
    "print(train_text_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Romove stopwords\n",
    "There are some words like `The`, `Of`, `a`, `an`, and so on. These words are stop words.  \n",
    "Generally, stop words should be removed to prevent them from affecting our results.  \n",
    "NLTK is shipped with stop words lists for most languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "stopwords_set = set(stopwords_list)\n",
    "\n",
    "# remove stopwords in training tokens\n",
    "train_clean_tokens = list()\n",
    "for token_list in train_text_tokens:\n",
    "    new_token_list = list()\n",
    "    for token in token_list:\n",
    "        if token in stopwords_set:\n",
    "            continue\n",
    "        new_token_list.append(token)\n",
    "        \n",
    "    train_clean_tokens.append(new_token_list)\n",
    "# print first record\n",
    "print(train_clean_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Stemming\n",
    "\n",
    "Word stemming means removing affixes from words and return the root word. Ex: The stem of the word working => work.  \n",
    "There are many algorithms for stemming, but the most used algorithm is **Porter stemming algorithm**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# a simple example\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('working'))\n",
    "print(stemmer.stem('increases'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# stemming for training data\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "train_stem_tokens = list()\n",
    "for token_list in train_clean_tokens:\n",
    "    new_token_list = [stemmer.stem(token)  for token in token_list]\n",
    "    train_stem_tokens.append(new_token_list)\n",
    "\n",
    "print(train_stem_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lemetization\n",
    "\n",
    "Word lemmatizing is similar to stemming, but the difference is the result of lemmatizing is a real word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# a simple example to compare stemming & lemmatization\n",
    "print(stemmer.stem('increases'))\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('increases'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# more examples for lemmatization\n",
    "print(lemmatizer.lemmatize('playing', pos=\"v\"))  # play is verb\n",
    "print(lemmatizer.lemmatize('playing', pos=\"n\"))  # play is noun\n",
    "print(lemmatizer.lemmatize('playing', pos=\"a\"))  # play is adjective\n",
    "print(lemmatizer.lemmatize('playing', pos=\"r\"))  # play is adverb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read & Preprocess text data for testing dataset\n",
    "\n",
    "Read testing dataset into pandas, we can put stopwords removing, stemming together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# read test data\n",
    "test_file = \"https://github.com/liuhoward/teaching/raw/master/big_data/smsspam/SMSSpamCollection.test\"\n",
    "test_data = pd.read_csv(test_file, sep='\\t', header=None, names=['label', 'text'])\n",
    "\n",
    "print(f'num test records: {len(test_data)}')\n",
    "\n",
    "test_texts = test_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize testing text\n",
    "test_text_tokens = [word_tokenize(text.lower())   for text in test_texts]\n",
    "\n",
    "test_stem_tokens = list()\n",
    "for token_list in test_text_tokens:\n",
    "    new_token_list = list()\n",
    "    for token in token_list:\n",
    "        # ignore stopwords\n",
    "        if token in stopwords_set:\n",
    "            continue\n",
    "        # stemming\n",
    "        new_token = stemmer.stem(token)\n",
    "        new_token_list.append(new_token)\n",
    "        \n",
    "    test_stem_tokens.append(new_token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generate features\n",
    "\n",
    "We should convert token sequences into numeric features for classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# token list to token dict\n",
    "train_tokens = [nltk.FreqDist(token_list) for token_list in train_stem_tokens]\n",
    "test_tokens = [nltk.FreqDist(token_list) for token_list in test_stem_tokens]\n",
    "\n",
    "# token dict to vector\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# define vectorizer\n",
    "feature_vectorizer = DictVectorizer()\n",
    "# learn token indices from training tokens\n",
    "feature_vectorizer.fit(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate feature vector for training:\n",
    "train_features = feature_vectorizer.transform(train_tokens)\n",
    "print(f'train features shape: {np.shape(train_features)}')\n",
    "# generate feature vector for testing:\n",
    "test_features = feature_vectorizer.transform(test_tokens)\n",
    "print(f'test features shape: {np.shape(test_features)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## generate labels\n",
    "\n",
    "Convert labels (ham & spam) to numeric indices (0, 1) for classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# get raw labels for training, testing data\n",
    "\n",
    "train_labels = train_data['label']\n",
    "\n",
    "test_labels = test_data['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# define label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# learn encoding of labels\n",
    "label_encoder.fit(train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# convert training labels into indices\n",
    "train_target = label_encoder.transform(train_labels)\n",
    "# shape of training label indices\n",
    "print(np.shape(train_target))\n",
    "# first 20 label indices\n",
    "print(train_target[0:20])\n",
    "\n",
    "# pay attention to unbalanced classes\n",
    "values, counts = np.unique(train_target, return_counts=True)\n",
    "print(f'unique values: {values}')\n",
    "print(f'unique values frequency: {counts}')\n",
    "\n",
    "# convert testing labels into indices\n",
    "test_target = label_encoder.transform(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classifier\n",
    "\n",
    "We use logistic regression classifier, you can try other classifiers like Naive Bayes, SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# define classifier\n",
    "# what if we remove class_weight?\n",
    "clf = LogisticRegression(solver='lbfgs', class_weight='balanced')\n",
    "\n",
    "clf.fit(X=train_features, y=train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# get predicted label indices of testing data\n",
    "test_pred = clf.predict(X=test_features)\n",
    "\n",
    "print(np.shape(test_pred))\n",
    "print(test_pred[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# get spam label index\n",
    "spam_index = int(label_encoder.transform(['spam'])[0])\n",
    "print(f'spam index is {spam_index}')\n",
    "\n",
    "accuracy = accuracy_score(test_target, test_pred)\n",
    "f1 = f1_score(test_target, test_pred, pos_label=spam_index)\n",
    "precision = precision_score(test_target, test_pred, pos_label=spam_index)\n",
    "recall = recall_score(test_target, test_pred, pos_label=spam_index)\n",
    "\n",
    "print(f'accuracy: {accuracy}')\n",
    "print(f'precision: {precision}')\n",
    "print(f'recall: {recall}')\n",
    "print(f'f1 score: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Q & A"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
